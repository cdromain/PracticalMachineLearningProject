---
title: "Qualitative physical activity recognition using sensors data"
subtitle: "Johns Hopkins D.S.S. - Course 8 Practical Machine Learning - Course project"
author: '[Romain Faure](https://github.com/cdromain)'
date: "March 2017"
output:
  html_document:
    css: ~/Documents/DataScience/Markdown/Custom CSS/Avenue Rom.css
    keep_md: yes
    number_sections: yes
    toc: yes
    toc_depth: 1
---

# Introduction

Using personal *wearable* monitoring devices, it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the *quantified self movement* â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health or find patterns in their behavior. In parallel, *Human Activity Recognition (HAR)* has emerged as a key research area in the last years.

Usually, people tend to quantify how much of a particular activity they do, or discriminate between different activities (i.e. predict which activity they do at a specific point in time), but they rarely quantify *how well* they perform a specific activity. 

In this project, our goal will be to use the Weight Lifting Exercises data set, i.e. data captured by **on-body sensors** that were placed on the belt, forearm, arm, and dumbell of $6$ male participants (aged between 20-28 years), to predict how they actually did the exercise. *Figure 1* below shows the experiment setup that was used ([source](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf)) :

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics("./index_files/figure-html/Sensing_setup.png")
```

The participants were asked to perform one set of $10$ repetitions of the unilateral dumbbell biceps curl (a biceps curl repetition involves raising and lowering the dumbbell) in $5$ different ways : correctly, i.e. exactly according to the specified execution of the exercise (class A), or incorrectly, like throwing the elbows to the front (class B), lifting the dumbbell only halfway (class C), lowering the dumbbell only halfway (class D) and throwing the hips to the front (class E).

The aim of this experiment was to investigate the feasibility of automatically assessing the quality of execution of weight lifting exercises - so-called **qualitative activity recognition**.

Therefore, we are here dealing with a **classification** problem. More specifically, our goal in this project is to build a prediction model capable of identifying the manner in which the participants did the exercise (i.e. the `classe` variable in the training set, our outcome), classifying every observation in one of the $5$ classes using the remaining variables in the training set.

> More information is available on this [website](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) and in this [paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf).

# Loading the data and the necessary packages

We start by downloading the data using the script below :

```{r, message=FALSE, warning=FALSE}
# Data downloading

## Check if a "data" directory exists in the current working directory
## and create it if not

if (!file.exists("data")) { 
        dir.create("data")
}

## Check if the 2 data files already exist in the "data" directory
## and download them if not

if (!file.exists("./data/pml-training.csv")) { 
        ## download the training data CSV file
        fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(fileUrl1, "./data/pml-training.csv", method = "curl")
}

if (!file.exists("./data/pml-testing.csv")) { 
        ## download the test data CSV file
        fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(fileUrl2, "./data/pml-testing.csv", method = "curl")
}
```

We then load the packages necessary for our analysis, set the seed for reproducibility purposes and read in the downloaded training and test data CSV files (specifying that empty cells should be interpreted as NA using `read.csv`'s `na.strings` argument) :

```{r, message=FALSE}
## Loading the necessary packages
library(caret)
library(randomForest)
library(VIM)
library(psych)
library(ggplot2)
library(parallel); library(doParallel)

## setting the seed
set.seed(777)

## Reading in the training and test data files
training <- read.csv(file = "./data/pml-training.csv", header = TRUE, na.strings = c("", "NA"))
test <- read.csv(file = "./data/pml-testing.csv", header = TRUE, na.strings = c("", "NA"))
```

# Exploratory Data Analysis

We start by checking the dimensions of the `training` data frame :

```{r, echo=FALSE}
cat("Numbers of rows : "); nrow(training)
cat("Numbers of columns : "); ncol(training)
```

Our training set has $19622$ observations (rows) and $160$ variables (columns).

We then check if the training set contains missing data (`NA`), using the `aggr` function from the [`VIM` package](https://cran.r-project.org/web/packages/VIM/index.html). This function outputs a plot showing the proportion of variables containing missing values, as well as a list of the variables sorted by number of missing values : 


```{r, echo=FALSE}
aggr(training, col=c('blue','red'), numbers=TRUE, sortVars=TRUE, only.miss = TRUE, labels=names(training), varheight = TRUE, combined = TRUE, cex.lab = 1, cex.axis = .2, cex.numbers = 0.8, ylab=c("Figure 2 : Missing data (red) in the training set"))
```

As we can see, a large proportion of the `training` data frame variables contain $97.9\%$ of missing values ($19216$) - the missing values are shown in red in *Figure 2* above.

We then use a `for` loop to list the variables of the training set which contain missing values, and store them in the `colNA` variable :

```{r}
## Select the columns with NAs or empty values
colNA <- c()

for(i in 1:ncol(training)) {
        if (sum(is.na(training[, i])) > 0) {
                colNA <- c(colNA, i)
        }
}
```

Among the `r length(training)` variables of the training set, `r length(colNA)`  contain missing values and only `r length(training) - length(colNA)` contain no missing value.

Finally we take a closer look at our outcome variable, `classe`, i.e. the variable that we'll be predicting :

```{r, echo=FALSE}
str(training$classe)
summary(training$classe)
```

`classe` is a **factor** variable with $5$ levels, `A`, `B`, `C`, `D` and `E`, corresponding to the $5$ ways of doing the exercise (see the introduction for more information).  

# Data pre-processing and features selection

We now know that our training set has `r length(colNA)` mostly empty columns. We can see these variables as *unsignificant noise* which would perturb our prediction models. Therefore, missing values in the training set should be handled before we can train our models, which means either imputing them or subsetting them out from the training set. Given the large number of mostly empty columns, imputation does not appear to be a relevant strategy, that's why we decide instead to **subset** the training set to only keep the complete columns with no missing value (in blue in the right hand side of *Figure 2* above), so as to use them as predictors when building our models.

We also decide to remove the first column of the training set, `X` (i.e. the row id number), as keeping it in our models might lead to overfitting - in other words, it would hinder our models generalization ability. 

> In the case of our test set, keeping the `X` variable as a predictor makes the model predict `A` for the $20$ test observations, which seems to be a clear symptom of overfitting (as the observations in the training set are ordered by `classe`, i.e. the first $20$ observations in the training set are all assigned to the class `A`).

We finally remove the second variable `user_name` as well, as it does not seem like a useful and relevant feature from a generalization and prediction point of view (new data might involve new users, i.e. new `user_name` values).

> We also considered removing the $3$ timestamps variables, as keeping them as predictors seems like it might limit the future generalization ability of our models. But as removing them slightly reduces the resulting models accuracy, we decide to keep them to maximize the accuracy in the specific context of this assignment.

This results in a new training set, `trainingOk`, containing only $58$ variables - our outcome, `classe`, and $57$ other variables that we'll use as predictors in our models :

```{r, echo=FALSE}
## Subsetting the training set to remove the columns with NAs 
## as well as the first two columns (X and user_name)
trainingOk <- training[, -c(1:2, colNA)]

# cat("Numbers of columns : "); ncol(trainingOk)
# cat("\n")
describe(trainingOk, skew = FALSE, ranges = FALSE)[1:2]
```

# Modeling strategy

## Algorithms short-list 

Given that we are dealing with a **non-binary classification** problem ($5$ classes), we decide to work with and compare the performance of three **tree-based ensembling** types of methods : *bagging* (model 1), *boosting* (model 2) and *random forest* (model 3).

1. `method = "treebag"` - bagged CART, more specifically bagging classification trees with bootstrap replications.

2. `method = "gbm"` - a stochastic gradient boosted model with multinomial loss function.

3. `method = "rf"` - random forest algorithm.

## Cross-validation

We decide to use **k-fold cross-validation** when building (i.e. training) our models to limit **overfitting** and get an estimate of the **out of sample (OOB, out-of-bag) accuracy**.

> Indeed, when doing **k-fold cross-validation**, for each fold the model is trained on $\frac{k - 1}{k}$ of the training data and then tested on the held-out (i.e. out-of-bag) remaining $\frac{1}{k}$. 

5-fold cross-validation seems sufficient in this context, as using a classical value of $k = 10$ basically more than doubles the processing time (except for the bagged CART) for only a marginal accuracy improvement with our three models ($+ 0.02\%$ with the bagged CART and $+ 0.01\%$ with GBM and random forest). See the measurements tables below comparing 5 and 10-fold cross-validation on the training set (using parallel processing on an Apple MacBookPro 15" 2013) :

- **5-fold cross-validation** :

| Model | Accuracy | Processing time (elapsed) |
|:---------|:---------:|:------:|
| 1 Bagged CART    | 0.9989808 | 54.4 sec |
| 2 GBM    | 0.9967895 | 155.0 sec |
| 3 Random forest    | 0.9992865 | 499.2 sec |
\newline

- **10-fold cross-validation** :

| Model | Accuracy | Processing time (elapsed) |
|:---------|:---------:|:------:|
| 1 Bagged CART    | 0.9991845 (*+ 0.0002037*) | 91.2 sec |
| 2 GBM    | 0.9968914 (*+ 0.0001019*) | 348.8 sec |
| 3 Random forest    | 0.9993884 (*+ 0.0001019*) | 1294.9 sec |

As a value of $k = 5$ already means more than $8$ minutes of computation for the random forest algorithm and yields satisfactory accuracy results, **5-fold cross-validation** seems to offer a good compromise between accuracy and processing time in this context. Therefore that's what we're going to use to train our models.

> According to the random forest creator, Leo Breiman, *"in random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run"* ([source](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr)). But to stay on the safe side and keep a common framework for our three models, we decide to use 5-fold cross-validation anyway with our three models, including the random forest-based model.

## Algorithms parameters

We choose to use the default parameters for our three models algorithms as these default values yield a satisfactory accuracy in this specific context.

## Parallel processing

We use **parallel processing** to speed up the processing time required to train our models. It's especially useful in this context since :
        
- The training set is fairly large (almost $20000$ observations).

- The algorithms we chose are fairly computationally intensive (especially random forest) as they are ensembling methods.

- We use 5-fold cross-validation.

- The computer the analysis is run on (Apple MacBookPro 15" 2013) has $4$ cores.
        
To enable parallel processing, we follow the [procedure described by the T.A. Len Greski](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md), using the packages `parallel` and `doParallel`.

# Computations and model selection

To evaluate and compare our three prediction models, we train them using `caret`'s `train` function to predict the `classe` variable in the training set using all the remaining variables :

```{r, cache=TRUE, message=FALSE}
## Configuring parallel processing
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

## Configuring trainControl object with parallel processing and 5-fold CV
fitControl <- trainControl(method = "cv", number = 5, 
                           allowParallel = TRUE)

## Building model 1 (bagged CART)
mod1bag <- train(classe ~ ., data = trainingOk, 
                 method = "treebag", trControl = fitControl)
pred1bag <- predict(mod1bag, trainingOk)
mod1bag # printing the resulting model

## Building model 2 (gbm)
mod2gbm <- train(classe ~ ., data = trainingOk, 
                 method = "gbm", trControl = fitControl, 
                 verbose = FALSE)
pred2gbm <- predict(mod2gbm, trainingOk)
mod2gbm

## Building model 3 (random forest)
mod3rf <- train(classe ~ ., data = trainingOk, 
                method = "rf", trControl = fitControl)
pred3rf <- predict(mod3rf, trainingOk)
mod3rf

## De-registering parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

We can then make a plot using `ggplot2` to compare the resulting accuracy and processing time of each of our three prediction models :

```{r}
modComp <- data.frame(x = c(mod1bag$times[[1]][3], mod2gbm$times[[1]][3], mod3rf$times[[1]][3]), 
                      y = c(mod1bag$results[[2]], mod2gbm$results[9, 5], mod3rf$results[2, 2]), 
                      model = c("1 treebag", "2 gbm", "3 rf"))

g <- ggplot(modComp, aes(x = x, y = y, colour = model))
g <- g + geom_point(size = 4)

g <- g + ggtitle("Figure 3 : Models comparison") 
g <- g + theme(plot.title = element_text(hjust=0.5, size = 11), 
               axis.title = element_text(size = 9)) + 
        labs(x = "Processing time (seconds)", y = "Model accuracy")
g
```

After looking at *Figure 3* above, we decide to use the **random forest** algorithm (`rf`) for our final prediction model as its accuracy is the highest, then followed by the bagged CART (`treebag`) and finally GBM. Note that random forest was also by far the most computationally intensive algorithm out of the three. 

> The bagged CART model could have been an interesting compromise as its accuracy is close to the random forest's, but only requires a small fraction of the computing time required by the random forest. But in the specific context of this assignment, we decide to stay with the most accurate model, i.e. the random forest.

# Results

The random forest algorithm selects the optimal model based on its accuracy. The final model which was selected by the algorithm used the following parameters : `mtry = 38` and `n.trees = 500`.

> The `mtry` parameter corresponds to the number of variables available for splitting at each tree node. ([source](http://code.env.duke.edu/projects/mget/export/HEAD/MGET/Trunk/PythonPackage/dist/TracOnlineDocumentation/Documentation/ArcGISReference/RandomForestModel.FitToArcGISTable.html))
>
> The `n.trees` parameter refers to the number of trees.

We can see in *Figure 4* below that `mtry = 38` indeed corresponds to the optimal parameter value according to the random forest algorithm random selection of predictors (increasing the parameter value beyond `38` decreases the resulting accuracy) :

```{r}
plot(mod3rf, main = list("Figure 4 : Random forest model predictors random selection", cex = 0.8))
```

Let's now check the accuracy of our random forest model using `caret`'s `confusionMatrix` function :

```{r}
confusionMatrix(pred3rf, trainingOk$classe)
```

As we can see, our random forest model enables us to correctly predict the `classe` of $100\%$ of the training observations (i.e. an in sample accuracy of $1$), with a $(0.9998, 1)$ $95\%$ confidence interval. 

Our random forest model also correctly identifies the `classe` of the $20$ test cases (out of sample) contained in the prediction quiz part of this assignment.

We can also plot our final model variables importance. *Figure 5* below shows the $20$ most important variables. The x-axis represents the total decrease in node impurities from splitting on the variable, averaged over all trees, as measured by the Gini index (as this is a classification task) : 

```{r}
varImpPlot(mod3rf$finalModel, n.var = 20, type = 2, cex = 0.7, color = "blue", main = "Figure 5 : Final model (random forest) variables importance")
```

Finally, let's look at our final random forest model in more details :

```{r}
mod3rf$finalModel
```

The resulting **out of sample (OOB) error** estimate of our final random forest model seems quite low ($0.05\%$), but we would probably need more than $6$ individuals in our training data to increase the generalization, i.e. the prediction ability of our model, as well as its *robustness*. We might also need a larger test set (i.e. containing more than $20$ observations) to definitely confirm the out of sample accuracy. 

# Credits

- This document was created in RStudio version 1.0.136, with R version 3.3.3, under Mac OSX 10.11.6.

- The HTML version was knitted using a customized version of the [Avenue CSS file](https://github.com/ttscoff/MarkedCustomStyles/blob/master/Avenue.css) by [Brett Terpstra](http://brettterpstra.com/).
