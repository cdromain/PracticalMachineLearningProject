---
title: "Predicting human physical activity using sensors data"
subtitle: "Johns Hopkins D.S.S. - Course 8 Practical Machine Learning - Course Project"
author: '[Romain Faure](https://github.com/cdromain) (R 3.3.3, RStudio 1.0.136, OSX 10.11.6)'
date: "March 2017"
output:
  html_document:
    keep_md: yes
    number_sections: yes
    toc: yes
    toc_depth: 1
---

# Introduction

Using personal *wearable* monitoring devices (such as Jawbone Up, Nike FuelBand, Fitbit...), it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the *quantified self movement* â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health or find patterns in their behavior. In parallel, Human Activity Recognition (HAR) has emerged as a key research area in the last years.

People tend to quantify how much of a particular activity they do, or discriminate between different activities, i.e. predict which activity they do at a specific point in time, but they rarely quantify how well they perform a specific activity. In this project, our goal will be to use the Weight Lifting Exercises Dataset, i.e. data from accelerometers sensors that were placed on the belt, forearm, arm, and dumbell of $6$ male participants (aged between 20-28 years), to predict how they actually did the exercise. 

The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in $5$ different ways : correctly, i.e. exactly according to the specified execution of the exercise (class A), or incorrectly, like throwing the elbows to the front (class B), lifting the dumbbell only halfway (class C), lowering the dumbbell only halfway (class D) and throwing the hips to the front (class E).

Therefore, we are here dealing with a **classification** problem. More specifically, our goal in this project is to build a prediction model capable of identifying the manner in which the participants did the exercise (i.e. the `classe` variable, our outcome), classifying every observation in one of the $5$ classes, using the remaining variables in the training set.

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises 

# Loading the data and necessary packages

We start by downloading the data using the script below :

```{r, message=FALSE, warning=FALSE}
## Data downloading

## check if a "data" directory exists in the current working directory
## and create it if not
downloaded <- FALSE

if (!file.exists("data")) { 
        dir.create("data")
}

if (!file.exists("./data/pml-training.csv")) { 
        dir.create("data")
        ## download the training data CSV file
        fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(fileUrl1, "./data/pml-training.csv", method = "curl")
        downloaded <- TRUE
}

if (!file.exists("./data/pml-testing.csv")) { 
        ## download the test data CSV file
        fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(fileUrl2, "./data/pml-testing.csv", method = "curl")
        downloaded <- TRUE
}


if (downloaded == TRUE) {
        ## save and print the download time and date
        downloadedDate <- date()
}

print("Files downloaded on :")
print(downloadedDate)
```

We then load the packages necessary for our analysis, set the seed for reproducibility purposes and read in the downloaded training and test data CSV files (specifying that empty cells should be interpreted as NAs) :

```{r, message=FALSE}
## Loading the necessary packages
library(caret)
library(randomForest)
library(VIM)
library(psych)
library(ggplot2)
library(parallel)
library(doParallel)

## setting the seed
set.seed(777)

## Reading in the training and test data files
training <- read.csv(file = "./data/pml-training.csv", header = TRUE, na.strings = c("", "NA"))
test <- read.csv(file = "./data/pml-testing.csv", header = TRUE, na.strings = c("", "NA"))
```

# Exploratory Data Analysis

```{r}
cat("Numbers of rows : "); nrow(training)
cat("Numbers of columns : "); ncol(training)
```


```{r}
aggr(training, col=c('blue','red'), numbers=TRUE, sortVars=TRUE, only.miss = TRUE, labels=names(training), varheight = TRUE, combined = TRUE, cex.lab = 1, cex.axis = .2, cex.numbers = 0.8, ylab=c("Figure 1 : Missing data (red) in the training set"))
```

```{r}
cat("\n classe variable :\n")
str(training$classe)
summary(training$classe)
```

3. Data Pre-processing and features selection

```{r}
## Select the columns with NAs or empty values
#trainingOk <- apply(training, 2, function(x) gsub("^$|^ $", NA, x))

colNA <- c()

for(i in 1:ncol(training)) {
        if (sum(is.na(training[,i])) > 0) {
                colNA <- c(colNA, i)
        }
}
```

We just saw that the training set has `r length(colNA)` empty columns, which we can see as silent (i.e. unsignificant) noise which would perturb our prediction models. Therefore, missing values in the training set should be handled before we can train our models, which means either imputing them or subsetting them out from the training set. Given the large number of mostly empty columns, imputation does not appear to be the best strategy, therefore we decide to subset the training set to only keep the complete columns (in blue in the plot above), using a `for` loop to determine which variables do not contain missing values, in order to use the complete columns as predictors to build our models.

We also remove the first column of the `training` data frame, `X`, (i.e. the row id number), as keeping it in our models would result in overfitting. In other words, it would hinder our model generalization ability. In the case of our test set, keeping the `X` variable as a predictor would lead the model to predict only `A` for the $20$ test observations, which seems to be a clear symptom of overfitting (as the observations in the training set are ordered by class, i.e. starting with `A`). We finally remove the second variable `user_name` as well, as it does not seem like a useful feature for generalization and prediction purposes (where new data would probably mean new users, i.e. new user names).

> We also considered removing the $3$ timestamps variables, as keeping them as predictors seems like it might limit the future generalization ability of our models. But as removing them slightly reduces the resulting models accuracy, we decide to keep them to maximize the accuracy in the specific context of this assignment.

This results in a new training set (`trainingOk`) containing only $58$ variables that we'll use as predictors :

```{r}
## Subsetting the training set to remove the columns with NAs 
## as well as the first column (X)
trainingOk <- training[, -c(1:2, colNA)]

cat("Numbers of columns : "); ncol(trainingOk)
cat("\n")
describe(trainingOk, skew = FALSE, ranges = FALSE)[1:2]
```

# Modeling strategy and model selection

- Given that we are dealing with a non-binary classification problem (5 classes), we decide to work with and compare the performance of  three tree-based ensembling types of methods : bagging (model 1), boosting (model 2) and random forest (model 3).

1. `method = "treebag"` - Bagged CART, more specifically bagging classification trees with bootstrap replications.

2. `method = "gbm"` - A stochastic gradient boosted model with multinomial loss function.

3. `method = "rf"` : Random forest algorithm.

- **Cross-validation** : we decide to use cross-validation as part of our models training to limit overfitting and get an OOB error rate estimate. 5-fold cross-validation seems enough, as using a classical value of `k = 10` doubles the processing time for only a marginal accuracy improvement with two of our three models : $+ 0.02\%$ with the bagged CART and the random forest algorithms but $- 0.03\%$ with GBM. As a value of $k = 5$ already means more than $7$ minutes of computation for the random forest algorithm (on the computer we're using) and satisfactory accuracy results, we decide to use 5-fold cross validation.

5 fold cross validation : accuracy / processing time (elapsed)
1 Treebag : 0.9989808 / 57.623 sec
2 GBM : 0.9970953 / 163.605 sec
3 RF : 0.9992866 / 480.586 sec

10 fold cross validation : accuracy / processing time (elapsed)
1 Treebag : 0.9991845 (+ 0.0002037) / 92.013 sec
2 GBM : 0.9968405 (- 0.0002548) / 299.202 sec
3 RF : 0.9994394 (+ 0.0001528) / 963.143 sec

- **Algorithms parameters** : using the default algorithms parameters led to a sufficient accuracy, so we did not change them.

- **Parallel processing** : we use parallel processing to speed up the processing time required to train our models. Especially useful since :
        
        - the training set is fairly large (almost $20000$ observations)
        - the algorithms we chose are fairly intensive (especially random forest)
        - the computer the analysis is run on has $4$ cores.
        
To enable parallel processing, we follow the [procedure described by the T.A. Len Greski](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md), using the packages `parallel` and `doParallel`.



```{r, cache=TRUE, message=FALSE}
## Configure parallel processing
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

## Configure trainControl object
fitControl <- trainControl(method = "cv", number = 5, 
                           allowParallel = TRUE)

## Building model 1 (bagged CART)
mod1bag <- train(classe ~ ., data = trainingOk, 
                 method = "treebag", trControl = fitControl)
pred1bag <- predict(mod1bag, trainingOk)
mod1bag # printing the resulting model

## Building model 2 (gbm)
mod2gbm <- train(classe ~ ., data = trainingOk, 
                 method = "gbm", trControl = fitControl, 
                 verbose = FALSE)
pred2gbm <- predict(mod2gbm, trainingOk)
mod2gbm

## Building model 3 (random forest)
mod3rf <- train(classe ~ ., data = trainingOk, 
                method = "rf", trControl = fitControl)
pred3rf <- predict(mod3rf, trainingOk)
mod3rf

## De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

We can then make a plot comparing the resulting accuracy and processing time of each of our three prediction models :

```{r}
modComp <- data.frame(x = c(mod1bag$times[[1]][3], mod2gbm$times[[1]][3], mod3rf$times[[1]][3]), y = c(mod1bag$results[[2]], mod2gbm$results[9, 5], mod3rf$results[2, 2]), model = c("1 treebag", "2 gbm", "3 rf"))

g <- ggplot(modComp, aes(x = x, y = y, colour = model))
g <- g + geom_point(size = 4)

g <- g + ggtitle("Figure 2 : Models comparison") 
g + theme(plot.title = element_text(hjust=0.5, size = 10), 
                 axis.title = element_text(size = 8)) + 
        labs(x = "Processing time (seconds)", y = "Model accuracy")
```

We decide to use a random forest algorithm (`rf`) for our final prediction model as its accuracy was the highest, then followed by the Bagged CART (`treebag`) and finally GBM. Note that random forest was also by far the most processing-intensive algorithm out of the three. The bagged CART model could have been an interesting compromise as its accuracy is close to the random forest's, but only requires one eighth of the computing time required by the random forest. But in the specific context of this assignment we decide to stay with the most accurate model, i.e. the random forest.

# Results

The final random forest model used the following parameters : `mtry = 38`, `n.trees = 150`, `interaction.depth = 3`, `shrinkage = 0.1` and `n.minobsinnode = 10`. 

We can see that `mtry = 38` corresponds to the optimal value according to the random forest algorithm predictors random selection (increasing the parameter value decreases the resulting accuracy) :

```{r}
plot(mod3rf, main = "Figure 3 : Final model predictors random selection")
```

Our final random forest-based model enables us to correctly predict the `classe` of $100\%$ of the training observations, with a $(0.9998, 1)$ confidence interval (as well as the $20$ test cases part of the prediction quiz) :

```{r}
confusionMatrix(pred3rf, trainingOk$classe)
```

Finally, we can plot our final model variables importance. The x-axis represents the total decrease in node impurities from splitting on the variable, averaged over all trees, and measured by the Gini index (as this is a classification task) : 

```{r}
varImpPlot(mod3rf$finalModel, n.var = 20, type = 2, cex = 0.7, color = "blue", main = "Figure 4 : Final model (random forest) variables importance")
```

```{r}
mod3rf$finalModel
```

Using **5-fold cross validation** when building our models gives us an estimate of the out of sample error (OOB, out-of-bag), as for each fold the model is trained on $\frac{4}{5}$ of the training data and then tested on the held-out remaining $\frac{1}{5}$. The resulting out of sample error (OOB) estimate seems quite low ($0.05\%$), but we would need more than $6$ individuals in our training data to increase the generalization, i.e. prediction abilities of our model. We might also need a larger test set containing more than $20$ observations to confirm the out of sample accuracy. 
